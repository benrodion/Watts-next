{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development \n",
    "\n",
    "In this Jupyter notebook, we will gradually develop our models. This entails:   \n",
    "- **preparatory work**: transforming data as needed (one-hot encoding, scaling...), train-test-split  \n",
    "- **model implementation**: creating baseline models  \n",
    "- **model tuning**: hyperparameter tuning to get the best version of each model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all sorts of packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_mon_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>wind_dir_avg_10</th>\n",
       "      <th>wind_speed_h_avg</th>\n",
       "      <th>wind_speed_avg_10</th>\n",
       "      <th>air_pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>full_datetime</th>\n",
       "      <th>capacity</th>\n",
       "      <th>volume</th>\n",
       "      <th>percentage</th>\n",
       "      <th>emission</th>\n",
       "      <th>emissionfactor</th>\n",
       "      <th>correct_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170101</td>\n",
       "      <td>1</td>\n",
       "      <td>207.708194</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>10234.526316</td>\n",
       "      <td>98.076923</td>\n",
       "      <td>2017-01-01-01</td>\n",
       "      <td>679334</td>\n",
       "      <td>679334</td>\n",
       "      <td>0.788730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170101</td>\n",
       "      <td>2</td>\n",
       "      <td>205.010321</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>10227.789474</td>\n",
       "      <td>98.153846</td>\n",
       "      <td>2017-01-01-02</td>\n",
       "      <td>677462</td>\n",
       "      <td>677462</td>\n",
       "      <td>0.786558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170101</td>\n",
       "      <td>3</td>\n",
       "      <td>202.701006</td>\n",
       "      <td>51.666667</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>10219.473684</td>\n",
       "      <td>98.230769</td>\n",
       "      <td>2017-01-01-03</td>\n",
       "      <td>653746</td>\n",
       "      <td>653746</td>\n",
       "      <td>0.759025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170101</td>\n",
       "      <td>4</td>\n",
       "      <td>201.007553</td>\n",
       "      <td>52.333333</td>\n",
       "      <td>54.666667</td>\n",
       "      <td>10211.368421</td>\n",
       "      <td>98.038462</td>\n",
       "      <td>2017-01-01-04</td>\n",
       "      <td>705882</td>\n",
       "      <td>705882</td>\n",
       "      <td>0.819552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170101</td>\n",
       "      <td>5</td>\n",
       "      <td>200.325015</td>\n",
       "      <td>52.666667</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>10203.526316</td>\n",
       "      <td>97.461538</td>\n",
       "      <td>2017-01-01-05</td>\n",
       "      <td>716738</td>\n",
       "      <td>716738</td>\n",
       "      <td>0.832158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_mon_day  hour  wind_dir_avg_10  wind_speed_h_avg  wind_speed_avg_10  \\\n",
       "0      20170101     1       207.708194         49.666667          49.666667   \n",
       "1      20170101     2       205.010321         50.000000          51.333333   \n",
       "2      20170101     3       202.701006         51.666667          51.000000   \n",
       "3      20170101     4       201.007553         52.333333          54.666667   \n",
       "4      20170101     5       200.325015         52.666667          53.333333   \n",
       "\n",
       "   air_pressure   humidity  full_datetime  capacity  volume  percentage  \\\n",
       "0  10234.526316  98.076923  2017-01-01-01    679334  679334    0.788730   \n",
       "1  10227.789474  98.153846  2017-01-01-02    677462  677462    0.786558   \n",
       "2  10219.473684  98.230769  2017-01-01-03    653746  653746    0.759025   \n",
       "3  10211.368421  98.038462  2017-01-01-04    705882  705882    0.819552   \n",
       "4  10203.526316  97.461538  2017-01-01-05    716738  716738    0.832158   \n",
       "\n",
       "   emission  emissionfactor   correct_days  \n",
       "0         0               0  2017-01-01-01  \n",
       "1         0               0  2017-01-01-02  \n",
       "2         0               0  2017-01-01-03  \n",
       "3         0               0  2017-01-01-04  \n",
       "4         0               0  2017-01-01-05  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "data_ons = pd.read_csv(\"data/final_onshore_data_2017_2025.csv\")\n",
    "data_ofs = pd.read_csv(\"data/final_offshore_data_2017_2025.csv\")\n",
    "\n",
    "# quick inspection\n",
    "data_ons.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split\n",
    "Since we are presumably going to work with k-fold cross-validation, we will not create a specific validation set. Instead, we will use a common 80/20 split. Before doing so, we of course store our label and our predictors seperately as X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X (features) and y (label)\n",
    "X_ons = data_ons.drop([\"volume\", \"capacity\"], #capacity & volume are the same\n",
    "                      axis=1)\n",
    "y_ons = data[\"volume\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split\n",
    "X_train_ons, X_test_ons, y_train_ons, y_test_ons = train_test_split(X_ons, y_ons, test_size=20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Pre-Processing\n",
    "For SARIMAX, categorical features need to be one-hot encoded, but since our data does not possess such feature, we can ignore this here. The data does not need to be scaled either. However, we need to define the **seasonal frequency** before we can build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement definition of seasonality here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ons = pd.read_csv(\"data/final_onshore_data_2017_2025.csv\")\n",
    "data_ofs = pd.read_csv(\"data/final_offshore_data_2017_2025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X (features) and y (label)\n",
    "X_ons = data_ons.drop([\"volume\", \"capacity\"], #capacity & volume are the same\n",
    "                      axis=1)\n",
    "y_ons = data[\"volume\"]\n",
    "\n",
    "X_train_ons, X_test_ons, y_train_ons, y_test_ons = train_test_split(X_ons, y_ons, test_size=20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "The data for LSTM **must** be scaled. We could scale via **normalization** or **standardization**, but with standardization, there is a caveat:  \n",
    "\"Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
    "Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\" [Brownlee](https://machinelearningmastery.com/how-to-scale-data-for-long-short-term-memory-networks-in-python/)  \n",
    "Therefore, we will opt for z-standarization here.\n",
    "\n",
    "When scaling, we must be careful not to scale any of the variables pertaining to time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_mon_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>wind_dir_avg_10</th>\n",
       "      <th>wind_speed_h_avg</th>\n",
       "      <th>wind_speed_avg_10</th>\n",
       "      <th>air_pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>full_datetime</th>\n",
       "      <th>percentage</th>\n",
       "      <th>emission</th>\n",
       "      <th>emissionfactor</th>\n",
       "      <th>correct_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170101</td>\n",
       "      <td>1</td>\n",
       "      <td>207.708194</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>10234.526316</td>\n",
       "      <td>98.076923</td>\n",
       "      <td>2017-01-01-01</td>\n",
       "      <td>0.788730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170101</td>\n",
       "      <td>2</td>\n",
       "      <td>205.010321</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>10227.789474</td>\n",
       "      <td>98.153846</td>\n",
       "      <td>2017-01-01-02</td>\n",
       "      <td>0.786558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170101</td>\n",
       "      <td>3</td>\n",
       "      <td>202.701006</td>\n",
       "      <td>51.666667</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>10219.473684</td>\n",
       "      <td>98.230769</td>\n",
       "      <td>2017-01-01-03</td>\n",
       "      <td>0.759025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170101</td>\n",
       "      <td>4</td>\n",
       "      <td>201.007553</td>\n",
       "      <td>52.333333</td>\n",
       "      <td>54.666667</td>\n",
       "      <td>10211.368421</td>\n",
       "      <td>98.038462</td>\n",
       "      <td>2017-01-01-04</td>\n",
       "      <td>0.819552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170101</td>\n",
       "      <td>5</td>\n",
       "      <td>200.325015</td>\n",
       "      <td>52.666667</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>10203.526316</td>\n",
       "      <td>97.461538</td>\n",
       "      <td>2017-01-01-05</td>\n",
       "      <td>0.832158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_mon_day  hour  wind_dir_avg_10  wind_speed_h_avg  wind_speed_avg_10  \\\n",
       "0      20170101     1       207.708194         49.666667          49.666667   \n",
       "1      20170101     2       205.010321         50.000000          51.333333   \n",
       "2      20170101     3       202.701006         51.666667          51.000000   \n",
       "3      20170101     4       201.007553         52.333333          54.666667   \n",
       "4      20170101     5       200.325015         52.666667          53.333333   \n",
       "\n",
       "   air_pressure   humidity  full_datetime  percentage  emission  \\\n",
       "0  10234.526316  98.076923  2017-01-01-01    0.788730         0   \n",
       "1  10227.789474  98.153846  2017-01-01-02    0.786558         0   \n",
       "2  10219.473684  98.230769  2017-01-01-03    0.759025         0   \n",
       "3  10211.368421  98.038462  2017-01-01-04    0.819552         0   \n",
       "4  10203.526316  97.461538  2017-01-01-05    0.832158         0   \n",
       "\n",
       "   emissionfactor   correct_days  \n",
       "0               0  2017-01-01-01  \n",
       "1               0  2017-01-01-02  \n",
       "2               0  2017-01-01-03  \n",
       "3               0  2017-01-01-04  \n",
       "4               0  2017-01-01-05  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define temporal features - we do not scale these\n",
    "time_features = [\"year_mon_day\", \"hour\", \"full_datetime\", \"correct_days\" ] \n",
    "\n",
    "# define numeric features to be scaled\n",
    "numeric_features = [\"wind_dir_avg_10\", \"wind_speed_h_avg\", \"wind_speed_avg_10\", \"air_pressure\", \"humidity\"]\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"date\", \"passthrough\", time_features),\n",
    "    (\"scaled_num\", StandardScaler(), numeric_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Transform the data with the scaler\n",
    "X_train_ons_norm = preprocessor.fit_transform(X_train_ons)\n",
    "X_test_ons_norm = preprocessor.fit_transform(X_test_ons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "Building a little toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ons = pd.read_csv(\"data/final_onshore_data_2017_2025.csv\")\n",
    "data_ofs = pd.read_csv(\"data/final_offshore_data_2017_2025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X (features) and y (label)\n",
    "X_ons = data_ons.drop([\"volume\", \"capacity\"], #capacity & volume are the same\n",
    "                      axis=1)\n",
    "y_ons = data[\"volume\"]\n",
    "\n",
    "X_train_ons, X_test_ons, y_train_ons, y_test_ons = train_test_split(X_ons, y_ons, test_size=20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Although normalization is not required for tree-based methods, it can enhance numerical stability, so let us implement it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define temporal features - we do not scale these\n",
    "time_features = [\"year_mon_day\", \"hour\", \"full_datetime\", \"correct_days\" ] \n",
    "\n",
    "# define numeric features to be scaled\n",
    "numeric_features = [\"wind_dir_avg_10\", \"wind_speed_h_avg\", \"wind_speed_avg_10\", \"air_pressure\", \"humidity\"]\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"date\", \"passthrough\", time_features),\n",
    "    (\"scaled_num\", StandardScaler(), numeric_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Transform the data with the scaler\n",
    "X_train_ons_norm = preprocessor.fit_transform(X_train_ons)\n",
    "X_test_ons_norm = preprocessor.fit_transform(X_test_ons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ons = pd.read_csv(\"data/final_onshore_data_2017_2025.csv\")\n",
    "data_ofs = pd.read_csv(\"data/final_offshore_data_2017_2025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X (features) and y (label)\n",
    "X_ons = data_ons.drop([\"volume\", \"capacity\"], #capacity & volume are the same\n",
    "                      axis=1)\n",
    "y_ons = data[\"volume\"]\n",
    "\n",
    "X_train_ons, X_test_ons, y_train_ons, y_test_ons = train_test_split(X_ons, y_ons, test_size=20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Again, normalization is not required but helps, so we are doing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define temporal features - we do not scale these\n",
    "time_features = [\"year_mon_day\", \"hour\", \"full_datetime\", \"correct_days\" ] \n",
    "\n",
    "# define numeric features to be scaled\n",
    "numeric_features = [\"wind_dir_avg_10\", \"wind_speed_h_avg\", \"wind_speed_avg_10\", \"air_pressure\", \"humidity\"]\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"date\", \"passthrough\", time_features),\n",
    "    (\"scaled_num\", StandardScaler(), numeric_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Transform the data with the scaler\n",
    "X_train_ons_norm = preprocessor.fit_transform(X_train_ons)\n",
    "X_test_ons_norm = preprocessor.fit_transform(X_test_ons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
