{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, time, datetime\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The date issue\n",
    "It is only possible to get a maximum of 200 data points per request from the API. It is hence necessary to find the data for every 200 hours between January 1st 2017 and January 1st 2025.\n",
    "\n",
    "The following code calculates the data and hour for every period lasting slightly less 200 hours between January 1st 2017 and January 1st 2025. We must convert this list back to a datetime.date object as the API request does not take specific hours. In the ideal world we would have looped through a list of list consisting of exactly 200 hour intervals e.g. [2017-01-01-00, 2017-01-09-08]. But now, we convert the list to normal dates again and then make a list of list with each object as a time interval of around 200 hrs fe. [2017-01-01, 2017-01-09].\n",
    "\n",
    " We can later loop through this list of lists but will have double hours in the data. These need to be removed during the data cleaning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    #for n in range(0, int((end_date - start_date).total_seconds()/3600), 200):\n",
    "    for n in range(0, int((end_date - start_date).days*24), 176):\n",
    "        yield start_date + timedelta(hours = n)\n",
    "         \n",
    "#create empty list to store dates\n",
    "datelist = []\n",
    "#define start and end date for list of dates\n",
    "start_day = date(2017, 1, 1)\n",
    "start_time = time(00)\n",
    "start_dt = datetime.combine(start_day, start_time)\n",
    "end_day = date(2025, 1, 1)\n",
    "end_time = time(00)\n",
    "end_dt = datetime.combine(end_day, end_time)\n",
    "#append dates to list\n",
    "for hrs in daterange(start_dt, end_dt):\n",
    "    hrs = hrs.strftime('%Y-%m-%d-%H')\n",
    "    datelist.append(hrs)\n",
    "\n",
    "len(datelist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_datelist = []\n",
    "for i in datelist:\n",
    "    date_format = '%Y-%m-%d-%H'\n",
    "    i = datetime.strptime(i, date_format).strftime('%Y-%m-%d')\n",
    "    dates_datelist.append(i)\n",
    "\n",
    "datelists = [item for item in dates_datelist for i in range(2)]\n",
    "datelists = datelists[1:]\n",
    "datelists.append(\"2025-01-02\")\n",
    "print(datelists[-1])\n",
    "#remove_double_end = datelists.pop(-1)\n",
    "out = [datelists[i: i+2] for i in range(0, len(datelists), 2)]\n",
    "print(len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to get all the offshore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.ned.nl/v1/utilizations?page=1&itemsPerPage=500&order%5Bvalidfrom%5D=desc\"\n",
    "Dataframes_all = []\n",
    "for i, j in out:\n",
    "    print(f\"Processing: {i} to {j}\")\n",
    "    headers_ij = {\n",
    "    'X-AUTH-TOKEN': 'ff760e9d4bacda20d72d33c6b06f8350310594eae88a05ba2268140dfe0469d8',\n",
    "    'accept': 'application/ld+json'}\n",
    "    params_ij = {'point': 14, 'type': 17, 'granularity': 5, 'granularitytimezone': 1, 'classification': 2, 'activity': 1,\n",
    " 'validfrom[strictly_before]': j, 'validfrom[after]': i}\n",
    "    response_ij = requests.get(url, headers=headers_ij, params=params_ij, allow_redirects=False)\n",
    "    sleep(3)\n",
    "    print(response_ij.status_code)\n",
    "    text_ij = response_ij.text\n",
    "    json_text_ij = json.loads(text_ij)\n",
    "\n",
    "    values_ij = [list(n.values()) for n in json_text_ij['hydra:member']]\n",
    "    headers_ij = list(json_text_ij['hydra:member'][0].keys())\n",
    "\n",
    "    # Create DataFrame\n",
    "    data_ij = pd.DataFrame(values_ij, columns=headers_ij)\n",
    "\n",
    "    # Append directly, without extra list nesting\n",
    "    Dataframes_all.append(data_ij)\n",
    "    print(f\"Dataframes list length: {len(Dataframes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.concat(Dataframes_all, ignore_index=True)\n",
    "print(final_result.shape)\n",
    "final_result.isnull().sum()\n",
    "#final_result.to_csv(\"All_Days_WOS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to get all the onshore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.ned.nl/v1/utilizations?page=1&itemsPerPage=500&order\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m5Bvalidfrom\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m5D=desc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m Dataframes_onsh \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[43mout\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     headers_ij \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX-AUTH-TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff760e9d4bacda20d72d33c6b06f8350310594eae88a05ba2268140dfe0469d8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/ld+json\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://api.ned.nl/v1/utilizations?page=1&itemsPerPage=500&order%5Bvalidfrom%5D=desc\"\n",
    "Dataframes_onsh = []\n",
    "for i, j in out:\n",
    "    print(f\"Processing: {i} to {j}\")\n",
    "    headers_ij = {\n",
    "    'X-AUTH-TOKEN': 'ff760e9d4bacda20d72d33c6b06f8350310594eae88a05ba2268140dfe0469d8',\n",
    "    'accept': 'application/ld+json'}\n",
    "    params_ij = {'point': 0, 'type': 1, 'granularity': 5, 'granularitytimezone': 1, 'classification': 2, 'activity': 1,\n",
    " 'validfrom[strictly_before]': j, 'validfrom[after]': i}\n",
    "    response_ij = requests.get(url, headers=headers_ij, params=params_ij, allow_redirects=False)\n",
    "    sleep(3)\n",
    "    print(response_ij.status_code)\n",
    "    text_ij = response_ij.text\n",
    "    json_text_ij = json.loads(text_ij)\n",
    "\n",
    "    values_ij = [list(n.values()) for n in json_text_ij['hydra:member']]\n",
    "    headers_ij = list(json_text_ij['hydra:member'][0].keys())\n",
    "\n",
    "    # Create DataFrame\n",
    "    data_ij = pd.DataFrame(values_ij, columns=headers_ij)\n",
    "\n",
    "    # Append directly, without extra list nesting\n",
    "    Dataframes_onsh.append(data_ij)\n",
    "    print(f\"Dataframes list length: {len(Dataframes_onsh)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_onsh = pd.concat(Dataframes_onsh, ignore_index=True)\n",
    "print(final_result_onsh.shape)\n",
    "final_result_onsh.isnull().sum()\n",
    "#final_result.to_csv(\"All_Days_WOS.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
